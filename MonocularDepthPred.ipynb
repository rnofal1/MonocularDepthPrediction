{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksk9usoEFzEB"
      },
      "source": [
        "# Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCWo6esZNudE"
      },
      "source": [
        "## IMPORTANT\n",
        "\n",
        "Before you can access the data, you need to save the folder \"E442-Final-Project-Data\" to your personal drive.\n",
        "\n",
        "To acces the data, right click the folder \"E442-Final-Project-Data\" in the same directory as this .ipynb and do this: \n",
        "\n",
        "> \"Add shortcut to Drive\" --> \"My Drive\" --> \"ADD SHORTCUT\"\n",
        "\n",
        "It must be in the root directory of your google drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRLWuk7DSaop"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rw7h-G6mGAmm"
      },
      "outputs": [],
      "source": [
        "# For mounting drive\n",
        "from google.colab import drive\n",
        "\n",
        "#For loading and saving data\n",
        "from os.path import exists\n",
        "import pickle\n",
        "!pip install mat73;\n",
        "import mat73;\n",
        "\n",
        "\n",
        "# For Splitting data\n",
        "import random\n",
        "import math\n",
        "\n",
        "# Other\n",
        "import cv2\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchsummary import summary\n",
        "\n",
        "from skimage.transform import resize\n",
        "from skimage import transform\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_tu2-BAyJ7M"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhZU5AbzSeZ5"
      },
      "source": [
        "## Mount Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dMCcCeGSlBw"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive/')\n",
        "path = 'drive/MyDrive/E442-Final-Project-Data/nyu_depth_data_labeled.mat'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgPLNvv8FzHS"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhvrRa4JKlRE"
      },
      "source": [
        "We use the NYUDepth dataset which can be found [here](https://cs.nyu.edu/~silberman/datasets/nyu_depth_v1.html). There is a NYUDepth v2 that we could potentially use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnNJFa8gEpJN"
      },
      "outputs": [],
      "source": [
        "data_dict = mat73.loadmat(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycLUPCcxMJY9"
      },
      "outputs": [],
      "source": [
        "# Access different data via dictionary\n",
        "print(\"Keys:\", list(data_dict.keys()))\n",
        "print(\"Depth Image Shape:\", data_dict['depths'].shape)\n",
        "print(\"Raw Image Shape:\", data_dict['images'].shape)\n",
        "print(\"Raw Depth Shape:\", data_dict['rawDepths'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHF-jdx9avH7"
      },
      "source": [
        "## Training/Testing Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8JasxJbVxrn"
      },
      "outputs": [],
      "source": [
        "# Initialize idxs for all data\n",
        "random.seed(0)\n",
        "data_size = data_dict['depths'].shape[-1]\n",
        "idxs = [i for i in range(data_size)]\n",
        "random.shuffle(idxs)\n",
        "\n",
        "# Parameters for splitting\n",
        "val_percent = 0.2\n",
        "small_size = 50\n",
        "med_size = 350\n",
        "large_size = 800\n",
        "full_size = data_size\n",
        "\n",
        "# Split for small dataset\n",
        "num_train_small = math.ceil(small_size * (1 - val_percent))\n",
        "num_val_small = math.floor(small_size * val_percent)\n",
        "train_small = idxs[:num_train_small]\n",
        "val_small = idxs[num_train_small : num_train_small + num_val_small]\n",
        "\n",
        "# Split for medium dataset\n",
        "num_train_med = math.ceil(med_size * (1 - val_percent))\n",
        "num_val_med = math.floor(med_size * val_percent)\n",
        "train_med = idxs[:num_train_med]\n",
        "val_med = idxs[num_train_med : num_train_med + num_val_med]\n",
        "\n",
        "# Split for large dataset\n",
        "num_train_large = math.ceil(large_size * (1 - val_percent))\n",
        "num_val_large = math.floor(large_size * val_percent)\n",
        "train_large = idxs[:num_train_large]\n",
        "val_large = idxs[num_train_large : num_train_large + num_val_large]\n",
        "\n",
        "# Split for full dataset\n",
        "num_train_full = math.ceil(full_size * (1 - val_percent))\n",
        "num_val_full = math.floor(full_size * val_percent)\n",
        "train_full = idxs[:num_train_full]\n",
        "val_full = idxs[num_train_full : num_train_full + num_val_full]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxwvI7MqGGZx"
      },
      "source": [
        "# Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJElVYQU5Epp"
      },
      "source": [
        "## Coarse Network\n",
        "\n",
        "See [our docs](https://docs.google.com/document/d/1rOpBCf5OhNuCuYNpwQqpdJqtWRRBqfEm73pKvBXMSSo/edit) for details about the networks implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xzf7ukkZhErV"
      },
      "outputs": [],
      "source": [
        "class Coarse_Network(nn.Module):\n",
        "  \"\"\"\n",
        "  See our docs for details about the networks implementation\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, dataset=\"NYUDepth\"):\n",
        "    \"\"\"\n",
        "    Builds the final layers 6 & 7 for the coarse network\n",
        "\n",
        "    This network was constructed from Fig. 1 of the paper, and the following\n",
        "    quote found in the same section:\n",
        "\n",
        "      \"The coarse-scale network contains five feature extraction layers of\n",
        "        convolution and max-pooling, followed by two fully connected layers.\"\n",
        "    \"\"\"\n",
        "    super(Coarse_Network, self).__init__()\n",
        "    self.dataset = dataset\n",
        "\n",
        "    # Build layers 1-5\n",
        "    self.coarse1 = nn.Sequential(\n",
        "      nn.Conv2d(3, 96, kernel_size=11, stride=4,padding=2),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2))\n",
        "    self.coarse2 = nn.Sequential(\n",
        "      nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2))\n",
        "    self.coarse3 = nn.Sequential(\n",
        "      nn.Conv2d(256, 384, kernel_size=3,padding=1),\n",
        "      nn.ReLU())\n",
        "    self.coarse4 = nn.Sequential(\n",
        "      nn.Conv2d(384, 384, kernel_size=3,padding=1),\n",
        "      nn.ReLU())\n",
        "    self.coarse5 = nn.Sequential(\n",
        "      nn.Conv2d(384, 256, kernel_size=3,stride=2), #was padding=(0,1)\n",
        "      nn.ReLU())\n",
        "\n",
        "    # Build layers 6-7\n",
        "    if self.dataset == \"NYUDepth\":\n",
        "      self.coarse6 = nn.Sequential(\n",
        "        nn.Linear(12288, 4096), # 12288 is 8x6x256\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5)\n",
        "      )\n",
        "      self.output = nn.Sequential(\n",
        "        nn.Linear(4096, 4070) # 4070 is 74x55\n",
        "      )\n",
        "    elif self.dataset == \"something else\":\n",
        "      pass\n",
        "    else:\n",
        "      raise ValueError(\"No valid dataset passed\")\n",
        "\n",
        "    # Init weights & biases\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "        m.bias.data.fill_(0)\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "  def forward(self, x): \n",
        "    \"\"\"\n",
        "    Forward pass through coarse network \n",
        "    \"\"\"\n",
        "    # Pass image through first 5 layers\n",
        "    x = self.coarse1(x)\n",
        "    x = self.coarse2(x)\n",
        "    x = self.coarse3(x)\n",
        "    x = self.coarse4(x)\n",
        "    x = self.coarse5(x)\n",
        "    # Pass image through last 2 layers\n",
        "    #x = torch.flatten(x, 1)\n",
        "    x = x.reshape(x.size(0), -1)\n",
        "    if self.dataset == \"NYUDepth\":\n",
        "      x = self.coarse6(x)\n",
        "      x = self.output(x)\n",
        "      x = x.reshape(x.size(0), 55, 74) # 6 is batch size\n",
        "\n",
        "    else:\n",
        "      raise ValueError(\"No valid dataset passed\")\n",
        "   \n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXjWOrsrRYDX"
      },
      "source": [
        "## Fine Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQfJY7PHV-3Q"
      },
      "outputs": [],
      "source": [
        "class Fine_Network(nn.Module):\n",
        "  def __init__(self, dataset=\"NYUDepth\"):\n",
        "    \"\"\"\n",
        "    Builds the layers for the fine network\n",
        "\n",
        "    This network was constructed from Fig. 1 of the paper\n",
        "\n",
        "    TODO:\n",
        "      - Confused by \"The last convolutional layer is linear\" in paper\n",
        "    \"\"\"\n",
        "    super(Fine_Network, self).__init__()\n",
        "    self.dataset = dataset\n",
        "\n",
        "    # Build layers 1 and 3\n",
        "    self.fine1 = nn.Sequential(\n",
        "      nn.Conv2d(3, 63, kernel_size=9, stride=2),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2))\n",
        "    \n",
        "    self.fine3 = nn.Sequential(\n",
        "      nn.Conv2d(64, 64, kernel_size=5,padding=2),\n",
        "      nn.ReLU())\n",
        "\n",
        "    # Build layer 4\n",
        "    if self.dataset == \"NYUDepth\":\n",
        "      self.fine4 = nn.Sequential(\n",
        "          nn.Conv2d(64, 1, kernel_size=5,padding=2))\n",
        "    elif self.dataset == \"something else\":\n",
        "      pass\n",
        "    else:\n",
        "      raise ValueError(\"No valid dataset passed\")\n",
        "\n",
        "    # Init weights & biases\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "        m.bias.data.fill_(0)\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "  def forward(self, x, coarse_out):\n",
        "    \"\"\"\n",
        "    Forward pass through the fine network\n",
        "    \"\"\"\n",
        "    # Pass image through first layer\n",
        "    x = self.fine1(x)\n",
        "\n",
        "    # Concat output of coarse network to output of fine1\n",
        "    x = torch.cat((x,coarse_out),dim=1)\n",
        "\n",
        "    # Pass image through third layer\n",
        "    x = self.fine3(x)\n",
        "   \n",
        "    # Pass image through final layer\n",
        "    if self.dataset == \"NYUDepth\":\n",
        "      x = self.fine4(x)\n",
        "      #x = x.view(-1, 55, 74) # 6 is batch size\n",
        "    else:\n",
        "      raise ValueError(\"No valid dataset passed\")\n",
        "   \n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaNCWHG-qvE2"
      },
      "source": [
        "# Train Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q211Y5Of5K2P"
      },
      "outputs": [],
      "source": [
        "def display_fine_epoch(input, coarse_output, fine_output, GT):\n",
        "  rows = 1\n",
        "  columns = 4 \n",
        "  fig = plt.figure(figsize=(12, 5))\n",
        "\n",
        "  # Adds a subplot at the 1st position\n",
        "  fig.add_subplot(rows, columns, 1)\n",
        "  \n",
        "  # Show image\n",
        "  plt.imshow(cv2.cvtColor(input, cv2.COLOR_BGR2RGB))\n",
        "  plt.axis('off')\n",
        "  plt.title(\"Input\")\n",
        "  \n",
        "  # Adds a subplot at the 2nd position\n",
        "  fig.add_subplot(rows, columns, 2)\n",
        "  \n",
        "  # Show image\n",
        "  plt.imshow(coarse_output,cmap='jet_r')\n",
        "  plt.axis('off')\n",
        "  plt.title(\"Coarse Output\")\n",
        "\n",
        "  # Adds a subplot at the 3rd position\n",
        "  fig.add_subplot(rows, columns, 3)\n",
        "\n",
        "  # Show image\n",
        "  plt.imshow(fine_output,cmap='jet_r')\n",
        "  plt.axis('off')\n",
        "  plt.title(\"Fine Output\")\n",
        "\n",
        "  # Adds a subplot at the 4th position\n",
        "  fig.add_subplot(rows, columns, 4)\n",
        "  \n",
        "  # Show image\n",
        "  plt.imshow(GT,cmap='jet_r')\n",
        "  plt.axis('off')\n",
        "  plt.title(\"Ground Truth\")\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def display_coarse_epoch(input, coarse_output, GT):\n",
        "  rows = 1\n",
        "  columns = 3 \n",
        "  fig = plt.figure(figsize=(10, 7))\n",
        "\n",
        "  # Adds a subplot at the 1st position\n",
        "  fig.add_subplot(rows, columns, 1)\n",
        "  \n",
        "  # Show image\n",
        "  plt.imshow(cv2.cvtColor(input, cv2.COLOR_BGR2RGB))\n",
        "  plt.axis('off')\n",
        "  plt.title(\"Input\")\n",
        "  \n",
        "  # Adds a subplot at the 2nd position\n",
        "  fig.add_subplot(rows, columns, 2)\n",
        "  \n",
        "  # Show image\n",
        "  plt.imshow(coarse_output,cmap='jet_r')\n",
        "  plt.axis('off')\n",
        "  plt.title(\"Coarse Output\")\n",
        "\n",
        "  # Adds a subplot at the 3rd position\n",
        "  fig.add_subplot(rows, columns, 3)\n",
        "\n",
        "  # Show image\n",
        "  plt.imshow(GT,cmap='jet_r')\n",
        "  plt.axis('off')\n",
        "  plt.title(\"Ground Truth\")\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXtZsssfGRBo"
      },
      "source": [
        "## Coarse Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1rSyDjxGYoC"
      },
      "outputs": [],
      "source": [
        "def train_coarse(coarse_net, dataloaders, criterion, optimizer, num_epochs):\n",
        "\n",
        "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.1,patience = 10)\n",
        "  epoch_loss_history = []\n",
        "  coarse_net.train()\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    # Images to be printed per epoch\n",
        "    input = None  # Input image of network\n",
        "    output = None # Output depth map of combined network\n",
        "    depth = None  # Ground truth depth map\n",
        "\n",
        "    for data in dataloaders:\n",
        "      # Save images to be printed\n",
        "      input = data['image'].to(device)\n",
        "      depth = data['depth'].to(device)\n",
        "      \n",
        "      output = coarse_net(input)\n",
        "      loss = criterion(output, depth, 0.5)\n",
        "      #loss = criterion(output, torch.log(depth))\n",
        "\n",
        "      coarse_net.zero_grad()\n",
        "      loss.backward() \n",
        "\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item() #* dataloaders.batch_size\n",
        "\n",
        "    # Calculate epoch loss\n",
        "    epoch_loss = running_loss / len(dataloaders)\n",
        "    epoch_loss_history.append(epoch_loss)\n",
        "\n",
        "    \n",
        "    scheduler.step(epoch_loss)\n",
        "    # Display images and loss\n",
        "    if not epoch % 10:\n",
        "      out_max = torch.max(output[0]).item()\n",
        "      print(\"Coarse Epoch Default (Log):\",epoch)\n",
        "      display_coarse_epoch(input[0].detach().cpu().numpy()[0], \n",
        "                    output[0].detach().cpu().numpy(), \n",
        "                    depth[0].detach().cpu().numpy())\n",
        "      print(\"Loss:\",epoch_loss,'\\n')\n",
        "\n",
        "      print(\"Coarse Epoch Exp:\",epoch)\n",
        "      display_coarse_epoch(input[0].detach().cpu().numpy()[0], \n",
        "                    np.exp(output[0].detach().cpu().numpy()), \n",
        "                    depth[0].detach().cpu().numpy())\n",
        "      print(\"Loss:\",epoch_loss,'\\n')\n",
        "\n",
        "\n",
        "  return epoch_loss_history \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y0ot-ssGla7"
      },
      "source": [
        "## Fine Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9bYwHV4GoeQ"
      },
      "outputs": [],
      "source": [
        "def train_fine(fine_net, coarse_net, dataloaders, criterion, optimizer, \n",
        "               num_epochs):\n",
        "  \n",
        "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.1,patience = 10)\n",
        "  epoch_loss_history = []\n",
        "  # Batches of images\n",
        "  input = None  # Input image of network\n",
        "  output = None # Output depth map of combined network\n",
        "  coarse_out = None\n",
        "  depth = None  # Ground truth depth map\n",
        "  epoch_loss = None\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for data in dataloaders:\n",
        "\n",
        "      coarse_net.eval()\n",
        "\n",
        "      # Save images to be printed\n",
        "      input = data['image'].to(device)\n",
        "      depth = data['depth'].to(device)\n",
        "\n",
        "      # Pass images through networks\n",
        "      with torch.no_grad():\n",
        "            coarse_out = coarse_net(input).unsqueeze(1)\n",
        "\n",
        "      output = fine_net(input, coarse_out).squeeze(1)\n",
        "      \n",
        "      # Update model\n",
        "      loss = criterion(output, depth, 0.5)\n",
        "      #loss = criterion(output, torch.log(depth))\n",
        "      \n",
        "      fine_net.zero_grad()\n",
        "\n",
        "      loss.backward() \n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item() #* dataloaders.batch_size\n",
        "\n",
        "    # Calculate epoch loss\n",
        "    epoch_loss = running_loss / len(dataloaders)\n",
        "    epoch_loss_history.append(epoch_loss)\n",
        "    \n",
        "    coarse_out = coarse_out.squeeze(1)\n",
        "    scheduler.step(epoch_loss)\n",
        "    # Display images and loss\n",
        "    if not epoch % 10:\n",
        "      print(\"Full Net Epoch Default (Log):\",epoch)\n",
        "      display_fine_epoch(input[0].detach().cpu().numpy()[0], \n",
        "                    coarse_out[0].detach().cpu().numpy(),\n",
        "                    output[0].detach().cpu().numpy(),\n",
        "                    depth[0].detach().cpu().numpy())\n",
        "      print(\"Loss:\",epoch_loss,'\\n')\n",
        "\n",
        "      print(\"Full Net Epoch Exp:\",epoch)\n",
        "      display_fine_epoch(input[0].detach().cpu().numpy()[0], \n",
        "                    np.exp(coarse_out[0].detach().cpu().numpy()),\n",
        "                    np.exp(output[0].detach().cpu().numpy()),\n",
        "                    depth[0].detach().cpu().numpy())\n",
        "      print(\"Loss:\",epoch_loss,'\\n')\n",
        "\n",
        "\n",
        "  # Display final loss and sample images\n",
        "  print(\"Final Full Epoch Default (Log):\",epoch)\n",
        "  display_fine_epoch(input[0].detach().cpu().numpy()[0], \n",
        "                    coarse_out[0].detach().cpu().numpy(),\n",
        "                    output[0].detach().cpu().numpy(),\n",
        "                    depth[0].detach().cpu().numpy())\n",
        "  display_fine_epoch(input[1].detach().cpu().numpy()[0], \n",
        "                    coarse_out[1].detach().cpu().numpy(),\n",
        "                    output[1].detach().cpu().numpy(),\n",
        "                    depth[1].detach().cpu().numpy())\n",
        "  display_fine_epoch(input[2].detach().cpu().numpy()[0], \n",
        "                    coarse_out[2].detach().cpu().numpy(),\n",
        "                    output[2].detach().cpu().numpy(),\n",
        "                    depth[2].detach().cpu().numpy())\n",
        "  display_fine_epoch(input[3].detach().cpu().numpy()[0], \n",
        "                    coarse_out[3].detach().cpu().numpy(),\n",
        "                    output[3].detach().cpu().numpy(),\n",
        "                    depth[3].detach().cpu().numpy())\n",
        "  print(\"Final Loss:\",epoch_loss,'\\n')\n",
        "\n",
        "  print(\"Final Full Epoch Exp:\",epoch)\n",
        "  display_fine_epoch(input[0].detach().cpu().numpy()[0], \n",
        "                    np.exp(coarse_out[0].detach().cpu().numpy()),\n",
        "                    np.exp(output[0].detach().cpu().numpy()),\n",
        "                    depth[0].detach().cpu().numpy())\n",
        "  display_fine_epoch(input[1].detach().cpu().numpy()[0], \n",
        "                    np.exp(coarse_out[1].detach().cpu().numpy()),\n",
        "                    np.exp(output[1].detach().cpu().numpy()),\n",
        "                    depth[1].detach().cpu().numpy())\n",
        "  display_fine_epoch(input[2].detach().cpu().numpy()[0], \n",
        "                    np.exp(coarse_out[2].detach().cpu().numpy()),\n",
        "                    np.exp(output[2].detach().cpu().numpy()),\n",
        "                    depth[2].detach().cpu().numpy())\n",
        "  display_fine_epoch(input[3].detach().cpu().numpy()[0], \n",
        "                    np.exp(coarse_out[3].detach().cpu().numpy()),\n",
        "                    np.exp(output[3].detach().cpu().numpy()),\n",
        "                    depth[3].detach().cpu().numpy())\n",
        "  print(\"Final Loss:\",epoch_loss,'\\n')\n",
        "  \n",
        "  return epoch_loss_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNdXJdnjHOPL"
      },
      "source": [
        "# Optimizer/Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4a5KQ0tHb8X"
      },
      "outputs": [],
      "source": [
        "def make_coarse_optimizer(model):\n",
        "    optimizer = optim.SGD([\n",
        "                {'params': model.coarse1.parameters()},\n",
        "                {'params': model.coarse2.parameters()},\n",
        "                {'params': model.coarse3.parameters()},\n",
        "                {'params': model.coarse4.parameters()},\n",
        "                {'params': model.coarse5.parameters()},\n",
        "                {'params': model.coarse6.parameters(), 'lr': 0.1},\n",
        "                {'params': model.output.parameters(), 'lr': 0.1}\n",
        "            ], lr=0.001, momentum=0.9,weight_decay=0.1)\n",
        "    return optimizer\n",
        "\n",
        "def make_fine_optimizer(model):\n",
        "    optimizer = optim.SGD([\n",
        "                {'params': model.fine1.parameters()},\n",
        "                {'params': model.fine3.parameters(), 'lr': 0.01},\n",
        "                {'params': model.fine4.parameters()}\n",
        "            ], lr=0.001, momentum=0.9,weight_decay=0.1)\n",
        "    return optimizer\n",
        "\n",
        "# Scale-invariant Mean Squared Error (in log space)\n",
        "\"\"\"\n",
        "def SIMSE(y_pred, y_true):\n",
        "  inner = torch.mean(torch.log(y_true ) - torch.log(y_pred))\n",
        "  outer = torch.mean( (torch.log(y_pred) - torch.log(y_true) + inner)**2)\n",
        "  return outer\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def SIMSE(y_pred, y_true, L):\n",
        "  mask = (y_true == 0) | (y_true == y_true.max()) | (y_true == y_true.min())\n",
        "  d = y_pred[~mask] - torch.log(y_true[~mask])\n",
        "  n = torch.numel(d)\n",
        "\n",
        "  d2 = torch.pow(d,2)\n",
        "  left = torch.sum(d2)/n\n",
        "  \n",
        "  right = L * torch.pow(torch.sum(d), 2)/ (n**2)\n",
        "  loss = left - right\n",
        "  return loss\n",
        "\n",
        "def Lin_SIMSE(y_pred, y_true):\n",
        "  print(\"Min GT:\",torch.min(y_true))\n",
        "  print(torch.min(y_pred))\n",
        "\n",
        "  print(\"Max GT:\",torch.max(y_true))\n",
        "  print(torch.max(y_pred))\n",
        "  L = 0.5\n",
        "  n = torch.numel(y_pred)\n",
        "  d = y_pred - y_true\n",
        "  d2 = torch.pow(d,2)\n",
        "  left = torch.sum(d2)/n\n",
        "  right = L * torch.pow(torch.sum(d), 2)/ (n**2)\n",
        "  loss = left - right\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmO1cz7OGtGS"
      },
      "source": [
        "# Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LP7PwaF_G5EO"
      },
      "outputs": [],
      "source": [
        "def plot_losses(x_range, vals, title):\n",
        "  x = np.arange(x_range)\n",
        "  plt.figure()\n",
        "  plt.plot(x, vals)\n",
        "  plt.legend(['Training Losses'])\n",
        "  plt.xticks(x)\n",
        "  plt.title(title)\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Epoch Loss')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Odzim2vQ4CO7"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zP9w5nuS3_Ob"
      },
      "outputs": [],
      "source": [
        "# Select images and depths from the data dictionary\n",
        "class DepthDataset(Dataset):\n",
        "  def __init__(self, data_idxs, transform=None, mode='train'):\n",
        "      self.transform = transform\n",
        "      self.data_idxs = data_idxs\n",
        "      self.mode = mode\n",
        "  def __len__(self):\n",
        "    return data_dict['depths'][:,:,self.data_idxs].shape[2]\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    image = data_dict['images'][:,:,:,self.data_idxs][:,:,:,idx]\n",
        "    depth = data_dict['depths'][:,:,self.data_idxs][:,:,idx]\n",
        "    label = data_dict['labels'][:,:,self.data_idxs][:,:,idx]\n",
        "\n",
        "    #sample = {'image': image, 'depth': depth, 'label' : label}\n",
        "    \n",
        "    \n",
        "    if self.mode == 'train':\n",
        "        \n",
        "        deg = random.uniform(-5.0,5.0)\n",
        "        flip = random.uniform(0,1)\n",
        "        \n",
        "        image = torchvision.transforms.functional.rotate(transforms.ToPILImage()(image),deg)\n",
        "        depth = torchvision.transforms.functional.rotate(transforms.ToPILImage()(depth),deg)\n",
        "\n",
        "        image = np.array(image)\n",
        "        depth = np.array(depth)\n",
        "\n",
        "        if flip < 0.5:\n",
        "          image = torchvision.transforms.functional.hflip(transforms.ToPILImage()(image))\n",
        "          depth = torchvision.transforms.functional.hflip(transforms.ToPILImage()(depth))\n",
        "\n",
        "\n",
        "        image = np.array(image)\n",
        "        depth = np.array(depth)\n",
        "    \n",
        "    sample = {'image': image, 'depth': depth, 'label' : label}\n",
        "    sample = self.transform(sample)\n",
        "\n",
        "    return sample\n",
        "\n",
        "\"\"\"\n",
        "Rescale and ToTensor are copied from the pytorch tutorial documentation:\n",
        "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "\"\"\"\n",
        "\n",
        "class Rescale(object):\n",
        "    \"\"\"Rescale the image in a sample to a given size.\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If tuple, output is\n",
        "            matched to output_size. If int, smaller of image edges is matched\n",
        "            to output_size keeping aspect ratio the same.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size,depth_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        assert isinstance(depth_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "        self.depth_size = depth_size\n",
        "        self.label_size = depth_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, depth, label = sample['image'], sample['depth'], sample['label']\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.output_size * h / w, self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "       \n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "\n",
        "        img = transform.resize(image, (new_h, new_w))\n",
        "        \n",
        "        h, w = depth.shape\n",
        "        if isinstance(self.depth_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.depth_size * h / w, self.depth_size\n",
        "            else:\n",
        "                new_h, new_w = self.depth_size, self.depth_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.depth_size\n",
        "\n",
        "        dep = transform.resize(depth, (new_h, new_w))\n",
        "\n",
        "        h, w = label.shape\n",
        "        if isinstance(self.label_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.label_size * h / w, self.label_size\n",
        "            else:\n",
        "                new_h, new_w = self.label_size, self.label_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.label_size\n",
        "\n",
        "        lab = transform.resize(label, (new_h, new_w))\n",
        "\n",
        "        return {'image': img, 'depth': dep, 'label' : lab}\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, depth, label = sample['image'], sample['depth'], sample['label']\n",
        "\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        return {'image': torch.from_numpy(np.float32(image)),\n",
        "                'depth': torch.from_numpy(np.float32(depth)),\n",
        "                'label': torch.from_numpy(np.float32(label))}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9j5maQp45fV"
      },
      "source": [
        "# Train Full"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SktxZaHXhVL"
      },
      "outputs": [],
      "source": [
        "# Train network\n",
        "model_path = 'drive/MyDrive/E442-Final-Project-Data/'\n",
        "ans = input(\"Do you want to train a new network?\")\n",
        "if ans == 'y':\n",
        "    print(\"Training a new network...\")\n",
        "    num_epochs = 41\n",
        "    criterion = SIMSE\n",
        "    #criterion = nn.MSELoss()\n",
        "\n",
        "    # Load Data\n",
        "    dataset = DepthDataset(train_large, \n",
        "                        transform=transforms.Compose([\n",
        "                                Rescale((228, 304),(55, 74)),\n",
        "                                ToTensor()]), mode='train')\n",
        "    \n",
        "    data_loader = DataLoader(dataset,batch_size=32,shuffle=True,num_workers=0)\n",
        "\n",
        "    # Train Coarse Network\n",
        "    coarseNet = Coarse_Network()\n",
        "    coarseNet.cuda()\n",
        "    coarse_optimizer = make_coarse_optimizer(coarseNet)\n",
        "    epoch_loss_history_coarse = train_coarse(coarseNet, data_loader, criterion, coarse_optimizer, num_epochs)\n",
        "\n",
        "    # Train Fine Network\n",
        "    fineNet = Fine_Network()\n",
        "    fineNet.cuda()\n",
        "    fine_optimizer = make_fine_optimizer(fineNet)\n",
        "    epoch_loss_history_fine = train_fine(fineNet, coarseNet, data_loader, criterion, fine_optimizer, num_epochs)\n",
        "\n",
        "    plot_losses(num_epochs, epoch_loss_history_coarse, \"NYUDepth_Coarse\")\n",
        "    plot_losses(num_epochs, epoch_loss_history_fine, \"NYUDepth_Combined\")\n",
        "\n",
        "    # Save network\n",
        "    ans = input(\"Do you want to save this model? (y/n)\")\n",
        "    if ans == 'y':\n",
        "        # Save current network\n",
        "        ans = ''\n",
        "        while ans == '':\n",
        "            ans = input(\"Enter model name\")\n",
        "        dataSet = input(\"Which dataset are you using? (Fx: NYUDepthV1)\")\n",
        "        torch.save({\n",
        "                    'epoch': num_epochs,\n",
        "                    'fine_model_state_dict': fineNet.state_dict(),\n",
        "                    'coarse_model_state_dict': coarseNet.state_dict(),\n",
        "                    'optimizer_state_dict': fine_optimizer.state_dict(),\n",
        "                    'loss': epoch_loss_history_fine,\n",
        "                    'dataSet': dataSet,\n",
        "                    'train_idxs': train_small,\n",
        "                    'val_idxs': val_small,\n",
        "                    }, model_path + ans + '.model')\n",
        "    model = fineNet\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acHY1TngCeWp"
      },
      "source": [
        "# Load and Validate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-95ptfxjinY"
      },
      "outputs": [],
      "source": [
        "checkpoint = None\n",
        "coarseNet = Coarse_Network().cuda()\n",
        "fineNet = Fine_Network().cuda()\n",
        "\n",
        "# Load in model\n",
        "ans = input(\"Do you want to load an existing model? (y/n)\")\n",
        "model_path = 'drive/MyDrive/E442-Final-Project-Data/'\n",
        "if ans == 'y':\n",
        "\n",
        "    criterion = SIMSE\n",
        "\n",
        "    # Find models\n",
        "    files = os.listdir(model_path)\n",
        "    models = [f[:-6] for f in files if f[-6:] == '.model']\n",
        "\n",
        "    # Pick model\n",
        "    if len(models) != 0:\n",
        "        print(models)\n",
        "        while ans not in models:\n",
        "            ans = input(\"Enter model: \")\n",
        "        checkpoint = torch.load(model_path + ans + '.model')\n",
        "    else:\n",
        "        print(\"There are no models...\")\n",
        "        sys.exit()\n",
        "\n",
        "    # Validation\n",
        "    ans = input(\"Do you want to test on the validation and training set? (y/n)\")\n",
        "    if ans == 'y':\n",
        "        # Transform for NYUDepth\n",
        "        trans = transforms.Compose([\n",
        "                                Rescale((228, 304),(55, 74)),\n",
        "                                ToTensor()])\n",
        "        \n",
        "        # Load model and set to eval for validation\n",
        "        coarseNet.load_state_dict(checkpoint['coarse_model_state_dict'])\n",
        "        fineNet.load_state_dict(checkpoint['fine_model_state_dict'])\n",
        "        coarseNet.eval()\n",
        "        fineNet.eval()\n",
        "\n",
        "        # Calculate training loss\n",
        "        # train_idxs = checkpoint['train_idxs']\n",
        "        train_idxs = train_large\n",
        "        \n",
        "        train_coarse_loss = []\n",
        "        train_fine_loss = []\n",
        "\n",
        "        # Create a data set from certain images and depths\n",
        "        #pretty surue dataset is effectively a subset of data_dict; only contains elements at the indices within train_idxs\n",
        "        #we honestly didn't need to bother with a dataloader like this? coulda just used loops and broadcasting and been fine? woulda been more straightforward?\n",
        "        dataset = DepthDataset(train_idxs, \n",
        "                        transform=transforms.Compose([\n",
        "                                Rescale((228, 304),(55, 74)),\n",
        "                                ToTensor()]),mode='test')\n",
        "        \n",
        "\n",
        "        # Create a DataLoader to properly access the data set\n",
        "        data_loader = DataLoader(dataset,batch_size=1,shuffle=False,num_workers=0)\n",
        "\n",
        "        for datums in data_loader:\n",
        "            # Extract image and depth\n",
        "            image = datums['image'].to(device)\n",
        "            depth = datums['depth'].to(device)\n",
        "\n",
        "            # Pass image/depth into networks\n",
        "            coarse_out = coarseNet(image)\n",
        "            fine_out = fineNet(image, coarse_out.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "            # Calculate loss, L=1 indicates SIError(see Eigen et al. top of pg5)\n",
        "            fine_loss = criterion(fine_out, depth, L=1)\n",
        "            train_fine_loss.append(fine_loss.item())\n",
        "            coarse_loss = criterion(coarse_out, depth, L=1)\n",
        "            train_coarse_loss.append(coarse_loss.item())\n",
        "\n",
        "        print(\"Average training loss of coarse network:\", np.array(train_coarse_loss).mean())\n",
        "        print(\"Average training loss of coarse + fine network:\", np.array(train_fine_loss).mean())\n",
        "\n",
        "        # Calculate validation loss\n",
        "        # val_idxs = checkpoint['val_idxs']\n",
        "        val_idxs = val_large\n",
        "        \n",
        "        val_coarse_loss = []\n",
        "        val_fine_loss = []\n",
        "        val_coarse_RMSE_loss = []\n",
        "        val_fine_RMSE_loss = []\n",
        "\n",
        "        val_input_img = []\n",
        "        val_coarse_img = []\n",
        "        val_fine_img = []\n",
        "        val_GT_img = []\n",
        "\n",
        "\n",
        "        # Create a data set from certain images and depths\n",
        "        #pretty surue dataset is effectively a subset of data_dict; only contains elements at the indices within train_idxs\n",
        "        #we honestly didn't need to bother with a dataloader like this? coulda just used loops and broadcasting and been fine? woulda been more straightforward?\n",
        "        dataset = DepthDataset(val_idxs, \n",
        "                        transform=transforms.Compose([\n",
        "                                Rescale((228, 304),(55, 74)),\n",
        "                                ToTensor()]),mode='test')\n",
        "        \n",
        "\n",
        "        # Create a DataLoader to properly access the data set\n",
        "        data_loader = DataLoader(dataset,batch_size=1,shuffle=False,num_workers=0)\n",
        "\n",
        "        for datums in data_loader:\n",
        "            # Extract image and depth\n",
        "            image = datums['image'].to(device)\n",
        "            depth = datums['depth'].to(device)\n",
        "\n",
        "            # Pass image/depth into networks\n",
        "            coarse_out = coarseNet(image)\n",
        "            fine_out = fineNet(image, coarse_out.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "            # Calculate loss, L=1 indicates SIError(see Eigen et al. top of pg5)\n",
        "            fine_loss = criterion(fine_out, depth, L=1)\n",
        "            val_fine_loss.append(fine_loss.item())\n",
        "            coarse_loss = criterion(coarse_out, depth, L=1)\n",
        "            val_coarse_loss.append(coarse_loss.item())\n",
        "\n",
        "            # Calculate loss for RMSE\n",
        "            fine_loss = torch.sqrt( nn.functional.mse_loss(fine_out, torch.log(depth)) )\n",
        "            val_fine_RMSE_loss.append(fine_loss.item())\n",
        "            coarse_loss = torch.sqrt( nn.functional.mse_loss(coarse_out, torch.log(depth)) )\n",
        "            val_coarse_RMSE_loss.append(coarse_loss.item())\n",
        "\n",
        "            # Save Output Images\n",
        "            val_input_img.append(image)\n",
        "            val_coarse_img.append(coarse_out)\n",
        "            val_fine_img.append(fine_out)\n",
        "            val_GT_img.append(depth)\n",
        "\n",
        "            # # Calculate loss\n",
        "            # loss = criterion(fine_out, depth, L=0.5)\n",
        "            # val_loss.append(loss.item())\n",
        "        \n",
        "        # Print losses\n",
        "        print(\"Average validation loss of coarse network:\", np.array(val_coarse_loss).mean())\n",
        "        print(\"Average validation loss of coarse + fine network:\", np.array(val_fine_loss).mean())\n",
        "        print(\"Average validation RMSE loss of coarse network:\", np.array(val_coarse_RMSE_loss).mean())\n",
        "        print(\"Average validation RMSE loss of coarse + fine network:\", np.array(val_fine_RMSE_loss).mean())\n",
        "\n",
        "\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jL2o8zbNjJTe"
      },
      "outputs": [],
      "source": [
        "def display_results(input, coarse_output, fine_output, GT):\n",
        "  rows = 1\n",
        "  columns = 4 \n",
        "  fig = plt.figure(figsize=(12, 5))\n",
        "\n",
        "  # Adds a subplot at the 1st position\n",
        "  fig.add_subplot(rows, columns, 1)\n",
        "  \n",
        "  # Show image\n",
        "  plt.imshow(cv2.cvtColor(input, cv2.COLOR_BGR2RGB))\n",
        "  plt.axis('off')\n",
        "  \n",
        "  # Adds a subplot at the 2nd position\n",
        "  fig.add_subplot(rows, columns, 2)\n",
        "  \n",
        "  # Show image\n",
        "  plt.imshow(coarse_output,cmap='jet_r')\n",
        "  plt.axis('off')\n",
        "\n",
        "  # Adds a subplot at the 3rd position\n",
        "  fig.add_subplot(rows, columns, 3)\n",
        "\n",
        "  # Show image\n",
        "  plt.imshow(fine_output,cmap='jet_r')\n",
        "  plt.axis('off')\n",
        "\n",
        "  # Adds a subplot at the 4th position\n",
        "  fig.add_subplot(rows, columns, 4)\n",
        "  \n",
        "  # Show image\n",
        "  plt.imshow(GT,cmap='jet_r')\n",
        "  plt.axis('off')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(\"Validation Output\")\n",
        "v_loss = np.array(val_loss)\n",
        "idx = np.argpartition(v_loss, 129)\n",
        "display_fine_epoch(val_input_img[48][0].detach().cpu().numpy()[0], \n",
        "                    np.exp(val_coarse_img[48][0][0].detach().cpu().numpy()),\n",
        "                    np.exp(val_fine_img[48][0].detach().cpu().numpy()),\n",
        "                    val_GT_img[48][0].detach().cpu().numpy())\n",
        "good_idx = [45,130,131,60,67,120,86,138,96]\n",
        "for i in good_idx:\n",
        "  #i = idx[k]\n",
        "  #print(i,\":\")\n",
        "  display_results(val_input_img[i][0].detach().cpu().numpy()[0], \n",
        "                    np.exp(val_coarse_img[i][0][0].detach().cpu().numpy()),\n",
        "                    np.exp(val_fine_img[i][0].detach().cpu().numpy()),\n",
        "                    val_GT_img[i][0].detach().cpu().numpy())\n",
        "\"\"\"\n",
        "print(np.mean(val_loss), np.std(val_loss), np.percentile(val_loss, 90))\n",
        "print(np.mean(train_loss), np.std(train_loss), np.percentile(train_loss, 90))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "ALT_E442FinalProject_V1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}